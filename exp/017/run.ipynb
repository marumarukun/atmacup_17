{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma:\n",
      "  model_path: unsloth/gemma-2-9b-it-bnb-4bit\n",
      "  metric: auc\n",
      "  max_length: 1024\n",
      "  fp16: true\n",
      "  learning_rate: 0.0001\n",
      "  epochs: 2\n",
      "  per_device_train_batch_size: 4\n",
      "  gradient_accumulation_steps: 16\n",
      "  per_device_eval_batch_size: 8\n",
      "  steps: 50\n",
      "  lr_scheduler_type: cosine\n",
      "  weight_decay: 0.01\n",
      "  optim: adamw_torch_fused\n",
      "  lora_r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.05\n",
      "  lora_bias: none\n",
      "exp_number: '017'\n",
      "run_name: base\n",
      "data:\n",
      "  data_root: ../../data\n",
      "  results_root: ../../results\n",
      "  train_path: ../../data/train.csv\n",
      "  cloth_path: ../../data/clothing_master.csv\n",
      "  test_path: ../../data/test.csv\n",
      "  sample_submission_path: ../../data/sample_submission.csv\n",
      "  results_dir: ../../results/017/base\n",
      "seed: 42\n",
      "n_splits: 4\n",
      "target: Recommended IND\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import safetensors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from hydra import compose, initialize\n",
    "from jinja2 import Template\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.seed import seed_everything\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "with initialize(config_path=\"config\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    cfg.exp_number = Path().resolve().name\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "pl.Config.set_fmt_str_lengths(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM(gemma-2-9b-it)をQLoRAでファインチューニング\n",
    "- 2nd & 7th place solを参考に実装\n",
    "  - https://www.guruguru.science/competitions/24/discussions/4f2c7270-b67e-4e34-855a-3246f03cc278/\n",
    "  - https://www.guruguru.science/competitions/24/discussions/bdfb41e9-a1ef-40e8-b67d-742b5a4458a2/\n",
    "- Gemma2-9B(instruct tuning)のSequenceClassification\n",
    "- 4bit量子化されたモデル(unsloth/gemma-2-9b-it-bnb-4bit)をQLoRAでファインチューニング\n",
    "- エラー発生のため1foldのみの学習に留まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_csv(cfg.data.train_path, try_parse_dates=True).fill_null(\"none\")\n",
    "test_df = pl.read_csv(cfg.data.test_path, try_parse_dates=True).fill_null(\"none\")\n",
    "cloth_df = pl.read_csv(cfg.data.cloth_path, try_parse_dates=True)\n",
    "\n",
    "train_df = train_df.join(cloth_df, on=\"Clothing ID\", how=\"left\")\n",
    "test_df = test_df.join(cloth_df, on=\"Clothing ID\", how=\"left\")\n",
    "\n",
    "# labels列を作成\n",
    "train_df = train_df.with_columns(pl.col(cfg.target).cast(pl.Int8).alias(\"labels\"))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug設定\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df.head(100)\n",
    "    test_df = test_df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Clothing ID</th><th>Age</th><th>Title</th><th>Review Text</th><th>Rating</th><th>Recommended IND</th><th>Positive Feedback Count</th><th>Division Name</th><th>Department Name</th><th>Class Name</th><th>labels</th><th>prompt</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i8</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>25</td><td>&quot;3-season skirt!&quot;</td><td>&quot;Adorable, well-made skirt! lined and very slimming. i had to size up b/c it runs a bit snug around t…</td><td>5</td><td>1</td><td>4</td><td>&quot;General&quot;</td><td>&quot;Bottoms&quot;</td><td>&quot;Skirts&quot;</td><td>1</td><td>&quot;This reviewer&#x27;s Age is 25.\n",
       "The Clothing Type is Skirts.\n",
       "The Review Title: 3-season skirt!\n",
       "The Review…</td></tr><tr><td>0</td><td>39</td><td>&quot;Very cute&quot;</td><td>&quot;Love the asymmetrical hem. waist fit snugly as in perfectly. it ties in two spots with a hidden zipp…</td><td>5</td><td>1</td><td>0</td><td>&quot;General&quot;</td><td>&quot;Bottoms&quot;</td><td>&quot;Skirts&quot;</td><td>1</td><td>&quot;This reviewer&#x27;s Age is 39.\n",
       "The Clothing Type is Skirts.\n",
       "The Review Title: Very cute\n",
       "The Review Text:…</td></tr><tr><td>0</td><td>42</td><td>&quot;Beautiful! fruns small for typical retailer sizing&quot;</td><td>&quot;I love this skirt! i wasn&#x27;t sure about the mix of the back pattern with the front but it works! it i…</td><td>5</td><td>1</td><td>5</td><td>&quot;General&quot;</td><td>&quot;Bottoms&quot;</td><td>&quot;Skirts&quot;</td><td>1</td><td>&quot;This reviewer&#x27;s Age is 42.\n",
       "The Clothing Type is Skirts.\n",
       "The Review Title: Beautiful! fruns small for…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 12)\n",
       "┌─────────────┬─────┬─────────────┬────────────┬───┬────────────┬────────────┬────────┬────────────┐\n",
       "│ Clothing ID ┆ Age ┆ Title       ┆ Review     ┆ … ┆ Department ┆ Class Name ┆ labels ┆ prompt     │\n",
       "│ ---         ┆ --- ┆ ---         ┆ Text       ┆   ┆ Name       ┆ ---        ┆ ---    ┆ ---        │\n",
       "│ i64         ┆ i64 ┆ str         ┆ ---        ┆   ┆ ---        ┆ str        ┆ i8     ┆ str        │\n",
       "│             ┆     ┆             ┆ str        ┆   ┆ str        ┆            ┆        ┆            │\n",
       "╞═════════════╪═════╪═════════════╪════════════╪═══╪════════════╪════════════╪════════╪════════════╡\n",
       "│ 0           ┆ 25  ┆ 3-season    ┆ Adorable,  ┆ … ┆ Bottoms    ┆ Skirts     ┆ 1      ┆ This       │\n",
       "│             ┆     ┆ skirt!      ┆ well-made  ┆   ┆            ┆            ┆        ┆ reviewer's │\n",
       "│             ┆     ┆             ┆ skirt!     ┆   ┆            ┆            ┆        ┆ Age is 25. │\n",
       "│             ┆     ┆             ┆ lined and  ┆   ┆            ┆            ┆        ┆ The        │\n",
       "│             ┆     ┆             ┆ very       ┆   ┆            ┆            ┆        ┆ Clothing   │\n",
       "│             ┆     ┆             ┆ slimming.  ┆   ┆            ┆            ┆        ┆ Type is    │\n",
       "│             ┆     ┆             ┆ i had to   ┆   ┆            ┆            ┆        ┆ Skirts.    │\n",
       "│             ┆     ┆             ┆ size up    ┆   ┆            ┆            ┆        ┆ The Review │\n",
       "│             ┆     ┆             ┆ b/c it     ┆   ┆            ┆            ┆        ┆ Title:     │\n",
       "│             ┆     ┆             ┆ runs a bit ┆   ┆            ┆            ┆        ┆ 3-season   │\n",
       "│             ┆     ┆             ┆ snug       ┆   ┆            ┆            ┆        ┆ skirt!     │\n",
       "│             ┆     ┆             ┆ around t…  ┆   ┆            ┆            ┆        ┆ The        │\n",
       "│             ┆     ┆             ┆            ┆   ┆            ┆            ┆        ┆ Review…    │\n",
       "│ 0           ┆ 39  ┆ Very cute   ┆ Love the   ┆ … ┆ Bottoms    ┆ Skirts     ┆ 1      ┆ This       │\n",
       "│             ┆     ┆             ┆ asymmetric ┆   ┆            ┆            ┆        ┆ reviewer's │\n",
       "│             ┆     ┆             ┆ al hem.    ┆   ┆            ┆            ┆        ┆ Age is 39. │\n",
       "│             ┆     ┆             ┆ waist fit  ┆   ┆            ┆            ┆        ┆ The        │\n",
       "│             ┆     ┆             ┆ snugly as  ┆   ┆            ┆            ┆        ┆ Clothing   │\n",
       "│             ┆     ┆             ┆ in         ┆   ┆            ┆            ┆        ┆ Type is    │\n",
       "│             ┆     ┆             ┆ perfectly. ┆   ┆            ┆            ┆        ┆ Skirts.    │\n",
       "│             ┆     ┆             ┆ it ties in ┆   ┆            ┆            ┆        ┆ The Review │\n",
       "│             ┆     ┆             ┆ two spots  ┆   ┆            ┆            ┆        ┆ Title:     │\n",
       "│             ┆     ┆             ┆ with a     ┆   ┆            ┆            ┆        ┆ Very cute  │\n",
       "│             ┆     ┆             ┆ hidden     ┆   ┆            ┆            ┆        ┆ The Review │\n",
       "│             ┆     ┆             ┆ zipp…      ┆   ┆            ┆            ┆        ┆ Text:…     │\n",
       "│ 0           ┆ 42  ┆ Beautiful!  ┆ I love     ┆ … ┆ Bottoms    ┆ Skirts     ┆ 1      ┆ This       │\n",
       "│             ┆     ┆ fruns small ┆ this       ┆   ┆            ┆            ┆        ┆ reviewer's │\n",
       "│             ┆     ┆ for typical ┆ skirt! i   ┆   ┆            ┆            ┆        ┆ Age is 42. │\n",
       "│             ┆     ┆ retailer    ┆ wasn't     ┆   ┆            ┆            ┆        ┆ The        │\n",
       "│             ┆     ┆ sizing      ┆ sure about ┆   ┆            ┆            ┆        ┆ Clothing   │\n",
       "│             ┆     ┆             ┆ the mix of ┆   ┆            ┆            ┆        ┆ Type is    │\n",
       "│             ┆     ┆             ┆ the back   ┆   ┆            ┆            ┆        ┆ Skirts.    │\n",
       "│             ┆     ┆             ┆ pattern    ┆   ┆            ┆            ┆        ┆ The Review │\n",
       "│             ┆     ┆             ┆ with the   ┆   ┆            ┆            ┆        ┆ Title:     │\n",
       "│             ┆     ┆             ┆ front but  ┆   ┆            ┆            ┆        ┆ Beautiful! │\n",
       "│             ┆     ┆             ┆ it works!  ┆   ┆            ┆            ┆        ┆ fruns      │\n",
       "│             ┆     ┆             ┆ it i…      ┆   ┆            ┆            ┆        ┆ small for… │\n",
       "└─────────────┴─────┴─────────────┴────────────┴───┴────────────┴────────────┴────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# promptを作成\n",
    "prompt_template = Template(\"\"\"This reviewer's Age is {{ age }}.\n",
    "The Clothing Type is {{ class_name }}.\n",
    "The Review Title: {{ title }}\n",
    "The Review Text: {{ review_text }}\n",
    ">. Will the reviewer recommend this cloth?\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def make_prompt_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    prompts = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        prompts.append(make_prompt(row))\n",
    "\n",
    "    df = df.with_columns(pl.Series(prompts).alias(\"prompt\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_prompt(row):\n",
    "    return prompt_template.render(\n",
    "        age=row[\"Age\"], class_name=row[\"Class Name\"], title=row[\"Title\"], review_text=row[\"Review Text\"]\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = make_prompt_column(train_df)\n",
    "test_df = make_prompt_column(test_df)\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model and tokenizer\n",
    "def setup_model_and_tokenizer():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.gemma.model_path)\n",
    "    tokenizer.add_eos_token = True\n",
    "    tokenizer.padding_side = \"right\"  # 文末に<eos>トークンを追加\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # <eos>をpad_tokenとして設定\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=cfg.gemma.lora_r,\n",
    "        lora_alpha=cfg.gemma.lora_alpha,\n",
    "        lora_dropout=cfg.gemma.lora_dropout,\n",
    "        bias=cfg.gemma.lora_bias,\n",
    "        inference_mode=False,\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma.model_path,\n",
    "        num_labels=2,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.config.use_cache = False  # キャッシュを使用しない\n",
    "    model = prepare_model_for_kbit_training(model)  # 量子化したモデルをファインチューニング可能にする\n",
    "    model = get_peft_model(model, peft_config)  # モデルにLoRAを適用\n",
    "    # model.print_trainable_parameters()\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"prompt\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5662aeae93341f2946fac1c40fa51c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d42dfaadff447a88c88f95d9b5e352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a668f63b39b41e4aae55dbcfe657256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 2:08:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.161682</td>\n",
       "      <td>0.974521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.195570</td>\n",
       "      <td>0.960731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.171763</td>\n",
       "      <td>0.976672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>0.977598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 AUC: 0.9775979776283826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m oof_auc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(train_df, y_train)):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Setup model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43msetup_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Setup dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     ds_train \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df[train_idx][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto_pandas())\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36msetup_model_and_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token  \u001b[38;5;66;03m# <eos>をpad_tokenとして設定\u001b[39;00m\n\u001b[1;32m     11\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     12\u001b[0m     r\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgemma\u001b[38;5;241m.\u001b[39mlora_r,\n\u001b[1;32m     13\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgemma\u001b[38;5;241m.\u001b[39mlora_alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     ],\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# キャッシュを使用しない\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model)  \u001b[38;5;66;03m# 量子化したモデルをファインチューニング可能にする\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3909\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3906\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3909\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3912\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# metricをAUCに変更\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = torch.softmax(torch.tensor(preds), dim=1).numpy()\n",
    "    score = roc_auc_score(labels, preds[:, 1])\n",
    "    return {\"auc\": score}\n",
    "\n",
    "\n",
    "# 実験結果格納用のディレクトリを作成\n",
    "cfg.run_name = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "Path(cfg.data.results_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "y_train = train_df[cfg.target].to_numpy()\n",
    "# oof = np.zeros(len(y_train))\n",
    "oof_auc = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y_train)):\n",
    "    # Setup model and tokenizer\n",
    "    model, tokenizer = setup_model_and_tokenizer()\n",
    "\n",
    "    # Setup dataset\n",
    "    ds_train = Dataset.from_pandas(train_df[train_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "    ds_val = Dataset.from_pandas(train_df[val_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "    ds_test = Dataset.from_pandas(test_df.select(\"prompt\").clone().to_pandas())\n",
    "\n",
    "    ds_train = ds_train.map(tokenize).remove_columns(\"prompt\")\n",
    "    ds_val = ds_val.map(tokenize).remove_columns(\"prompt\")\n",
    "    ds_test = ds_test.map(tokenize).remove_columns(\"prompt\")\n",
    "\n",
    "    # Setup trainer\n",
    "    output_dir = os.path.join(cfg.data.results_dir, f\"fold{fold}\")\n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=output_dir,  # 学習結果の出力ディレクトリ\n",
    "        fp16=cfg.gemma.fp16,  # 16ビット浮動小数点演算を使用するかどうか\n",
    "        learning_rate=cfg.gemma.learning_rate,  # 学習率\n",
    "        num_train_epochs=cfg.gemma.epochs,  # 学習エポック数\n",
    "        per_device_train_batch_size=cfg.gemma.per_device_train_batch_size,  # デバイスあたりの訓練バッチサイズ\n",
    "        per_device_eval_batch_size=cfg.gemma.per_device_eval_batch_size,  # デバイスあたりの評価バッチサイズ\n",
    "        gradient_accumulation_steps=cfg.gemma.gradient_accumulation_steps,  # 勾配蓄積ステップ数\n",
    "        gradient_checkpointing=True,  # 勾配チェックポイントを使用するかどうか\n",
    "        report_to=\"none\",  # レポート出力先（なし）\n",
    "        evaluation_strategy=\"steps\",  # 評価戦略（ステップごと）\n",
    "        do_eval=True,  # 評価を行うかどうか\n",
    "        eval_steps=cfg.gemma.steps,  # 評価を行うステップ間隔\n",
    "        save_total_limit=1,  # 保存するモデルの最大数\n",
    "        save_strategy=\"steps\",  # 保存戦略（ステップごと）\n",
    "        save_steps=cfg.gemma.steps,  # モデルを保存するステップ間隔\n",
    "        logging_steps=cfg.gemma.steps,  # ログを出力するステップ間隔\n",
    "        load_best_model_at_end=True,  # 学習終了時に最良のモデルをロードするかどうか\n",
    "        lr_scheduler_type=cfg.gemma.lr_scheduler_type,  # 学習率スケジューラーの種類\n",
    "        metric_for_best_model=cfg.gemma.metric,  # 最良モデルを判断するための評価指標\n",
    "        greater_is_better=True,  # 評価指標が大きいほど良いかどうか\n",
    "        warmup_ratio=0.1,  # ウォームアップの比率\n",
    "        weight_decay=cfg.gemma.weight_decay,  # 重み減衰\n",
    "        save_safetensors=True,  # SafeTensorsフォーマットで保存するかどうか\n",
    "        seed=cfg.seed,  # 乱数シード\n",
    "        data_seed=cfg.seed,  # データシャッフル用の乱数シード\n",
    "        optim=cfg.gemma.optim,  # 最適化アルゴリズム\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Perform inference on val and test datasets\n",
    "    pred_val = torch.softmax(torch.tensor(trainer.predict(ds_val).predictions), dim=1).numpy()[:, 1]\n",
    "    pred_test = torch.softmax(torch.tensor(trainer.predict(ds_test).predictions), dim=1).numpy()[:, 1]\n",
    "\n",
    "    # Save the model, predictions\n",
    "    final_output_dir = f\"{cfg.data.results_dir}/fold{fold}/final\"\n",
    "    trainer.save_model(final_output_dir)\n",
    "    np.save(f\"{final_output_dir}/val.npy\", pred_val)\n",
    "    np.save(f\"{final_output_dir}/test.npy\", pred_test)\n",
    "    # tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "    # Calculate and log AUC score\n",
    "    roc_auc = roc_auc_score(y_train[val_idx], pred_val)\n",
    "    print(f\"Fold {fold} AUC: {roc_auc}\")\n",
    "    oof_auc.append(roc_auc)\n",
    "\n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(f\"Mean AUC score across all folds: {np.mean(oof_auc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target</th></tr><tr><td>f32</td></tr></thead><tbody><tr><td>0.999411</td></tr><tr><td>0.590307</td></tr><tr><td>0.999469</td></tr><tr><td>0.175185</td></tr><tr><td>0.998186</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 1)\n",
       "┌──────────┐\n",
       "│ target   │\n",
       "│ ---      │\n",
       "│ f32      │\n",
       "╞══════════╡\n",
       "│ 0.999411 │\n",
       "│ 0.590307 │\n",
       "│ 0.999469 │\n",
       "│ 0.175185 │\n",
       "│ 0.998186 │\n",
       "└──────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提出\n",
    "sub_df = pl.read_csv(cfg.data.sample_submission_path)\n",
    "sub_df = sub_df.with_columns(pl.Series(pred_test).alias(\"target\"))\n",
    "sub_df.write_csv(os.path.join(cfg.data.results_dir, f\"{cfg.run_name}_submission.csv\"))\n",
    "sub_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
