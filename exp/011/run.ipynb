{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert:\n",
      "  params:\n",
      "    model_path: microsoft/deberta-v3-large\n",
      "    metric: auc\n",
      "    target_col_class_num: 2\n",
      "    max_length: 180\n",
      "    fp16: true\n",
      "    learning_rate: 1.0e-05\n",
      "    epochs: 3\n",
      "    per_device_train_batch_size: 8\n",
      "    per_device_eval_batch_size: 16\n",
      "    steps: 25\n",
      "    lr_scheduler_type: cosine\n",
      "    weight_decay: 0.01\n",
      "exp_number: '011'\n",
      "run_name: base\n",
      "data:\n",
      "  data_root: ../../data\n",
      "  results_root: ../../results\n",
      "  train_path: ../../data/train.csv\n",
      "  cloth_path: ../../data/clothing_master.csv\n",
      "  test_path: ../../data/test.csv\n",
      "  sample_submission_path: ../../data/sample_submission.csv\n",
      "  results_dir: ../../results/011/base\n",
      "seed: 42\n",
      "n_splits: 5\n",
      "target: Recommended IND\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.seed import seed_everything\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "with initialize(config_path=\"config\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    cfg.exp_number = Path().resolve().name\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp010をベースに下記変更\n",
    "- lr_scheduler_typeをlinear --> cosineに変更\n",
    "- learning_rateを2e-5 --> 1e-5に変更\n",
    "- epochsを2 --> 3に変更\n",
    "- stepsを50 --> 25に変更\n",
    "- Smooth Focal Lossを使用\n",
    "  - https://www.kaggle.com/c/nbme-score-clinical-patient-notes/discussion/322799#1781528 を参考に実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test_df = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df.head(1000)\n",
    "\n",
    "\n",
    "def preprocess_text(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # df = df.with_columns(pl.col(\"Title\").fill_null(\"none\"), pl.col(\"Review Text\").fill_null(\"none\"))\n",
    "    df = df.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.lit(\"TITLE: \"),\n",
    "                pl.col(\"Title\").fill_null(\"none\"),\n",
    "                pl.lit(\" [SEP] \"),\n",
    "                pl.lit(\" Review Text: \"),\n",
    "                pl.col(\"Review Text\").fill_null(\"none\"),\n",
    "            ]\n",
    "        ).alias(\"prompt\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = preprocess_text(train_df)\n",
    "test_df = preprocess_text(test_df)\n",
    "\n",
    "train_df = train_df.with_columns(pl.col(cfg.target).cast(pl.Int8).alias(\"labels\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.bert.params.model_path)\n",
    "\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"prompt\"], max_length=cfg.bert.params.max_length, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: 167\n",
      "test_df: 166\n"
     ]
    }
   ],
   "source": [
    "# token長を確認 --> max_length 180とかで大丈夫そう\n",
    "print(\n",
    "    f\"train_df: {train_df.select(pl.col('prompt').map_elements(lambda x: len(tokenizer(x)['input_ids']))).max().item()}\"\n",
    ")\n",
    "print(\n",
    "    f\"test_df: {test_df.select(pl.col('prompt').map_elements(lambda x: len(tokenizer(x)['input_ids']))).max().item()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoth Focal Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"none\", alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = self.alpha * (1.0 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SmoothFocalLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\", alpha=1, gamma=2, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.focal_loss = FocalLoss(reduction=\"none\", alpha=alpha, gamma=gamma)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets: torch.Tensor, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n",
    "        loss = self.focal_loss(inputs, targets)\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_fn=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if self.loss_fn is None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            # ロジットとラベルの形状を調整\n",
    "            logits = logits[:, 1]  # ポジティブクラスのロジットのみを使用\n",
    "            labels = labels.float()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Smooth Focal Lossのインスタンスを作成\n",
    "loss_fn = SmoothFocalLoss(alpha=1, gamma=2, smoothing=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94d41957bad4227a2a75fe4abaedfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b69850822444bf0810dbb3532f65b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 20:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.131209</td>\n",
       "      <td>0.561222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.121198</td>\n",
       "      <td>0.797175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.066375</td>\n",
       "      <td>0.945495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.118543</td>\n",
       "      <td>0.942155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>0.965918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.064227</td>\n",
       "      <td>0.963221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.064295</td>\n",
       "      <td>0.964176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.050910</td>\n",
       "      <td>0.968017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>0.969657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>0.971889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.048623</td>\n",
       "      <td>0.972313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.051684</td>\n",
       "      <td>0.970961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.061286</td>\n",
       "      <td>0.964120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.053848</td>\n",
       "      <td>0.969418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.058077</td>\n",
       "      <td>0.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.055468</td>\n",
       "      <td>0.972084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.046573</td>\n",
       "      <td>0.973389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.052378</td>\n",
       "      <td>0.971140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.048965</td>\n",
       "      <td>0.972370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.047628</td>\n",
       "      <td>0.971624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.055078</td>\n",
       "      <td>0.971191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.049610</td>\n",
       "      <td>0.973252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.053879</td>\n",
       "      <td>0.972085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.053134</td>\n",
       "      <td>0.973713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>0.973763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>0.972628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.973302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.052353</td>\n",
       "      <td>0.973263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.052358</td>\n",
       "      <td>0.973144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.052382</td>\n",
       "      <td>0.973508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2312e99a221746efa9a5ed8c4d83cb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbd46747eda4c9b93852c37c77b2d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 20:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.127652</td>\n",
       "      <td>0.610652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.095856</td>\n",
       "      <td>0.905804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.078383</td>\n",
       "      <td>0.946109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.061190</td>\n",
       "      <td>0.956729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.056174</td>\n",
       "      <td>0.967151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.057538</td>\n",
       "      <td>0.958149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.055047</td>\n",
       "      <td>0.969962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.059612</td>\n",
       "      <td>0.964999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.055441</td>\n",
       "      <td>0.963552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.053407</td>\n",
       "      <td>0.971019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.053506</td>\n",
       "      <td>0.963552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050097</td>\n",
       "      <td>0.969465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.060069</td>\n",
       "      <td>0.964380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.058086</td>\n",
       "      <td>0.962863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.966387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.054662</td>\n",
       "      <td>0.971591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.053799</td>\n",
       "      <td>0.968549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>0.968550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.055907</td>\n",
       "      <td>0.965561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.055051</td>\n",
       "      <td>0.968628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.061071</td>\n",
       "      <td>0.967416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.068693</td>\n",
       "      <td>0.961571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.961193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.959939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.064430</td>\n",
       "      <td>0.959387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>0.960931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.065310</td>\n",
       "      <td>0.960911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.063757</td>\n",
       "      <td>0.962109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.063376</td>\n",
       "      <td>0.962758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.962512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9dea7f97414bd8baf9cf66a1d7553a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df050da69f7f4126bbb6fee01c33740d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 21:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.599429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.109922</td>\n",
       "      <td>0.883775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.066859</td>\n",
       "      <td>0.953709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.059642</td>\n",
       "      <td>0.962641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.054156</td>\n",
       "      <td>0.966121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.060009</td>\n",
       "      <td>0.970971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.054108</td>\n",
       "      <td>0.965501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.056255</td>\n",
       "      <td>0.956133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>0.963338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.056756</td>\n",
       "      <td>0.966678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.967705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.060948</td>\n",
       "      <td>0.970221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.057879</td>\n",
       "      <td>0.972647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.059920</td>\n",
       "      <td>0.969451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.051285</td>\n",
       "      <td>0.971831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.055943</td>\n",
       "      <td>0.970359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.055726</td>\n",
       "      <td>0.971836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.972058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.052474</td>\n",
       "      <td>0.970920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.059374</td>\n",
       "      <td>0.966428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.062189</td>\n",
       "      <td>0.964881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>0.966624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.059285</td>\n",
       "      <td>0.963504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.060631</td>\n",
       "      <td>0.967321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.063032</td>\n",
       "      <td>0.962267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.064296</td>\n",
       "      <td>0.961884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.064889</td>\n",
       "      <td>0.960440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.961406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.961551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.961537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4902ced8397d43d5845704cbb86619b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c106dff70c4543bc59917e932df6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 20:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.128514</td>\n",
       "      <td>0.570873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.116932</td>\n",
       "      <td>0.791867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.075045</td>\n",
       "      <td>0.919322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.072648</td>\n",
       "      <td>0.922988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.071935</td>\n",
       "      <td>0.951904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.061084</td>\n",
       "      <td>0.955929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.080873</td>\n",
       "      <td>0.956003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.057381</td>\n",
       "      <td>0.960653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>0.963331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.055822</td>\n",
       "      <td>0.964294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.062996</td>\n",
       "      <td>0.962595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.075217</td>\n",
       "      <td>0.961164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.056299</td>\n",
       "      <td>0.963557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.965561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.054870</td>\n",
       "      <td>0.968602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.050990</td>\n",
       "      <td>0.967998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.969543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.969315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.968569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.968098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.967414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.058719</td>\n",
       "      <td>0.968256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.060278</td>\n",
       "      <td>0.967503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.058691</td>\n",
       "      <td>0.968807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.058845</td>\n",
       "      <td>0.969347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.060177</td>\n",
       "      <td>0.968818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.060095</td>\n",
       "      <td>0.969444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.060084</td>\n",
       "      <td>0.969056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.059690</td>\n",
       "      <td>0.969404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.059679</td>\n",
       "      <td>0.969013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6b361e835c4901b211435cd5c249d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e898fdcc0849cc9b280bb0a86fe487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 20:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.128870</td>\n",
       "      <td>0.581892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.837154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.077662</td>\n",
       "      <td>0.928163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.079395</td>\n",
       "      <td>0.966810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.053428</td>\n",
       "      <td>0.966815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.048719</td>\n",
       "      <td>0.973224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.971420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.054134</td>\n",
       "      <td>0.971587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.053307</td>\n",
       "      <td>0.966603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.051936</td>\n",
       "      <td>0.971908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.962704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.051454</td>\n",
       "      <td>0.971387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.060179</td>\n",
       "      <td>0.967506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.049832</td>\n",
       "      <td>0.970897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.047347</td>\n",
       "      <td>0.975871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.050677</td>\n",
       "      <td>0.974242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.050055</td>\n",
       "      <td>0.973161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.057691</td>\n",
       "      <td>0.973294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.050464</td>\n",
       "      <td>0.974082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.057110</td>\n",
       "      <td>0.973385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.063714</td>\n",
       "      <td>0.968252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>0.970728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.063493</td>\n",
       "      <td>0.969765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.062686</td>\n",
       "      <td>0.971828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.058816</td>\n",
       "      <td>0.971361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.062681</td>\n",
       "      <td>0.969877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.064190</td>\n",
       "      <td>0.970052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>0.970159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.064480</td>\n",
       "      <td>0.970423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.064449</td>\n",
       "      <td>0.970472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# metricをAUCに変更\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = torch.softmax(torch.tensor(preds), dim=1).numpy()\n",
    "    score = roc_auc_score(labels, preds[:, 1])\n",
    "    return {\"auc\": score}\n",
    "\n",
    "\n",
    "# 実験結果格納用のディレクトリを作成\n",
    "cfg.run_name = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "Path(cfg.data.results_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "y_train = train_df[cfg.target].to_numpy()\n",
    "oof = np.zeros(len(y_train))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y_train)):\n",
    "    ds_train = Dataset.from_pandas(train_df[train_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "    ds_val = Dataset.from_pandas(train_df[val_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "\n",
    "    config = AutoConfig.from_pretrained(cfg.bert.params.model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(cfg.bert.params.model_path, config=config)\n",
    "\n",
    "    ds_train = ds_train.map(tokenize).remove_columns(\"prompt\")\n",
    "    ds_val = ds_val.map(tokenize).remove_columns(\"prompt\")\n",
    "\n",
    "    output_dir = os.path.join(cfg.data.results_dir, f\"fold{fold}\")\n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        fp16=cfg.bert.params.fp16,\n",
    "        learning_rate=cfg.bert.params.learning_rate,\n",
    "        num_train_epochs=cfg.bert.params.epochs,\n",
    "        per_device_train_batch_size=cfg.bert.params.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=cfg.bert.params.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        eval_steps=cfg.bert.params.steps,\n",
    "        save_total_limit=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=cfg.bert.params.steps,\n",
    "        logging_steps=cfg.bert.params.steps,\n",
    "        load_best_model_at_end=True,\n",
    "        lr_scheduler_type=cfg.bert.params.lr_scheduler_type,\n",
    "        metric_for_best_model=cfg.bert.params.metric,\n",
    "        greater_is_better=True,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=cfg.bert.params.weight_decay,\n",
    "        save_safetensors=True,\n",
    "        seed=cfg.seed,\n",
    "        data_seed=cfg.seed,\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    final_output_dir = f\"{cfg.data.results_dir}/fold{fold}/final\"\n",
    "    trainer.save_model(final_output_dir)\n",
    "    tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "    pred = torch.softmax(torch.tensor(trainer.predict(ds_val).predictions), dim=1).numpy()\n",
    "    oof[val_idx] = pred[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af42c08505f748a79f5fffcc3510e139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88a52fa53ea42f1bdd2e5b1ebd008f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04bb9f3a7464905871a02c242890fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dd54a879da4a83bcfa79f844e3028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6963356af89d463a9c807fb7ee6ad456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "for fold in range(cfg.n_splits):\n",
    "    # ベストステップのモデルを取得\n",
    "    fold_dir = f\"{cfg.data.results_dir}/fold{fold}\"\n",
    "    checkpoint_dirs = [d for d in os.listdir(fold_dir) if d.startswith(\"checkpoint-\")]\n",
    "    results_dir = os.path.join(fold_dir, checkpoint_dirs[0])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(results_dir)\n",
    "\n",
    "    ds_test = Dataset.from_pandas(test_df.select(\"prompt\").clone().to_pandas())\n",
    "    ds_test = ds_test.map(tokenize).remove_columns(\"prompt\")\n",
    "\n",
    "    test_args = TrainingArguments(\n",
    "        output_dir=cfg.data.results_dir,\n",
    "        per_device_eval_batch_size=cfg.bert.params.per_device_eval_batch_size,\n",
    "        do_predict=True,\n",
    "        dataloader_drop_last=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=test_args,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    predictions = torch.softmax(torch.tensor(trainer.predict(ds_test).predictions), dim=1).numpy()\n",
    "    preds.append(predictions[:, 1])\n",
    "\n",
    "pred = np.mean(preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target</th></tr><tr><td>f32</td></tr></thead><tbody><tr><td>0.938488</td></tr><tr><td>0.43351</td></tr><tr><td>0.931818</td></tr><tr><td>0.301179</td></tr><tr><td>0.940782</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 1)\n",
       "┌──────────┐\n",
       "│ target   │\n",
       "│ ---      │\n",
       "│ f32      │\n",
       "╞══════════╡\n",
       "│ 0.938488 │\n",
       "│ 0.43351  │\n",
       "│ 0.931818 │\n",
       "│ 0.301179 │\n",
       "│ 0.940782 │\n",
       "└──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提出\n",
    "sub_df = pl.read_csv(cfg.data.sample_submission_path)\n",
    "sub_df = sub_df.with_columns(pl.Series(pred).alias(\"target\"))\n",
    "sub_df.write_csv(os.path.join(cfg.data.results_dir, f\"{cfg.run_name}_submission.csv\"))\n",
    "sub_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oof\n",
    "- smoothingいらなかったっぽい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.histplot(y_train, bins=50)\n",
    "sns.histplot(oof, bins=50)\n",
    "plt.legend([\"true\", \"oof\"])\n",
    "plt.show()\n",
    "fig.savefig(f\"{cfg.data.results_dir}/oof_hist.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOFを保存\n",
    "np.save(f\"{cfg.data.results_dir}/oof.npy\", oof)\n",
    "\n",
    "# 予測結果も保存\n",
    "np.save(f\"{cfg.data.results_dir}/pred.npy\", pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
